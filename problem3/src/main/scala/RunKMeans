// ------------------------ Disable excessive logging -------------------------

import org.apache.log4j.Logger
import org.apache.log4j.Level

Logger.getLogger("org").setLevel(Level.ERROR)
Logger.getLogger("akka").setLevel(Level.ERROR)

// ---------------- Our First Take on K-Means Clustering --------------------------

import org.apache.spark.mllib.clustering._
import org.apache.spark.mllib.linalg._
import org.apache.spark.rdd._
import org.apache.spark.{ SparkConf, SparkContext }
import org.apache.spark.SparkContext._

val rawData = sc.textFile("../Data/kddcup.corrected.csv").sample(false, 0.01)
rawData.map(_.split(',').last).countByValue().toSeq.sortBy(_._2).
  reverse.foreach(println)

val labelsAndData = rawData.map { line =>
  val buffer = line.split(',').toBuffer
  buffer.remove(1, 3)
  val label = buffer.remove(buffer.length - 1)
  val vector = Vectors.dense(buffer.map(_.toDouble).toArray)
  (label, vector)
}

val data = labelsAndData.values.cache()

val kmeans = new KMeans()
val model = kmeans.run(data)

model.clusterCenters.foreach(println)

val clusterLabelCount = labelsAndData.map {
  case (label, vector) =>
    val cluster = model.predict(vector)
    (cluster, label)
}.countByValue()

clusterLabelCount.toSeq.sorted.foreach {
  case ((cluster, label), count) =>
    println(f"$cluster%1s$label%18s$count%8s")
}

// ------------ Use this to Dump the Non-Normalized Data & Clusters -----------

val kmeans = new KMeans()
kmeans.setK(100)
val model = kmeans.run(data)
val sample = data.map(vector =>
  model.predict(vector) + "," + vector.toArray.mkString(",")).sample(false, 0.01)
sample.saveAsTextFile("./kmeans-sample")

// ------------ Investigate the Average Distance to Closest Centroid  ---------

def distance(a: Vector, b: Vector) =
  math.sqrt(a.toArray.zip(b.toArray).map(p => p._1 - p._2).map(d => d * d).sum)

def distToCentroid(vector: Vector, model: KMeansModel) = {
  val cluster = model.predict(vector)
  val centroid = model.clusterCenters(cluster)
  distance(centroid, vector)
}

def clusteringScore(data: RDD[Vector], k: Int): Double = {
  val kmeans = new KMeans()
  kmeans.setK(k)
  val model = kmeans.run(data)
  data.map(vector => distToCentroid(vector, model)).mean()
}

(30 to 60 by 10).par.map(k => (k, clusteringScore(data, k))).
  foreach(println)

// ------------ Normalize the Data and Compare Clustering Scores --------------

def normalize(data: RDD[Vector]): (Vector => Vector) = {
  val dataAsArray = data.map(_.toArray)
  val numCols = dataAsArray.first().length
  val n = dataAsArray.count()
  val sums = dataAsArray.reduce(
    (a, b) => a.zip(b).map(t => t._1 + t._2))
  val sumSquares = dataAsArray.aggregate(
    new Array[Double](numCols))(
      (a, b) => a.zip(b).map(t => t._1 + t._2 * t._2),
      (a, b) => a.zip(b).map(t => t._1 + t._2))
  val stdevs = sumSquares.zip(sums).map {
    case (sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n
  }
  val means = sums.map(_ / n)

  (vector: Vector) => {
    val normalizedArray = (vector.toArray, means, stdevs).zipped.map(
      (value, mean, stdev) =>
        if (stdev <= 0) (value - mean) else (value - mean) / stdev)
    Vectors.dense(normalizedArray)
  }
}

val normalizedData = data.map(normalize(data)).cache()
normalizedData.cache()
(60 to 120 by 10).par.map(k =>
  (k, clusteringScore(normalizedData, k))).toList.foreach(println)
normalizedData.unpersist()

// ------------ Switch to One-Hot Encoding of Categorical Attributes  ---------

def onehot(rawData: RDD[String]): (String => (String, Vector)) = {
  val splitData = rawData.map(_.split(','))
  val protocols = splitData.map(_(1)).distinct().collect().zipWithIndex.toMap
  println("PROTOCOLS")
  protocols.foreach(println)
  val services = splitData.map(_(2)).distinct().collect().zipWithIndex.toMap
  println("SERVICES")
  services.foreach(println)
  val tcpStates = splitData.map(_(3)).distinct().collect().zipWithIndex.toMap
  println("TCP_STATES")
  tcpStates.foreach(println)
  (line: String) => {
    val buffer = line.split(',').toBuffer
    val protocol = buffer.remove(1)
    val service = buffer.remove(1)
    val tcpState = buffer.remove(1)
    val label = buffer.remove(buffer.length - 1)
    val vector = buffer.map(_.toDouble)

    val newProtocolFeatures = new Array[Double](protocols.size)
    newProtocolFeatures(protocols(protocol)) = 1.0
    val newServiceFeatures = new Array[Double](services.size)
    newServiceFeatures(services(service)) = 1.0
    val newTcpStateFeatures = new Array[Double](tcpStates.size)
    newTcpStateFeatures(tcpStates(tcpState)) = 1.0

    vector.insertAll(1, newTcpStateFeatures)
    vector.insertAll(1, newServiceFeatures)
    vector.insertAll(1, newProtocolFeatures)

    (label, Vectors.dense(vector.toArray))
  }
}

// ------------- Dump the Normalized and One-Hot Encoded Data  ----------------

val parseFunction = onehot(rawData)
val labelsAndData = rawData.map(parseFunction)
val normalizedLabelsAndData =
  labelsAndData.mapValues(normalize(labelsAndData.values)).cache()
val kmeans = new KMeans()
kmeans.setK(100)
val model = kmeans.run(normalizedLabelsAndData.values)
val sample = normalizedLabelsAndData.values.map(vector =>
  model.predict(vector) + "," + vector.toArray.mkString(",")).sample(false, 0.01)
sample.saveAsTextFile("./kmeans-sample-normalized")
normalizedLabelsAndData.unpersist()

// ------------ Switch to Entropy-Based Clustering Scores  --------------------

def entropy(counts: Iterable[Int]) = {
  val values = counts.filter(_ > 0)
  val n: Double = values.sum
  values.map { v =>
    val p = v / n
    -p * math.log(p)
  }.sum
}

def clusteringScoreByEntropy(normalizedData: RDD[(String, Vector)], k: Int) = {

  normalizedData.cache()

  val kmeans = new KMeans()
  kmeans.setK(k)
  val model = kmeans.run(normalizedData.values)

  normalizedData.unpersist()

  val labelsAndClusters = normalizedData.mapValues(model.predict)
  val clustersAndLabels = labelsAndClusters.map(_.swap)
  val labelsInCluster = clustersAndLabels.groupByKey().values
  val labelCounts = labelsInCluster.map(_.groupBy(l => l).map(_._2.size))
  val n = normalizedData.count()

  labelCounts.map(m => m.sum * entropy(m)).sum / n
}

val parseFunction = onehot(rawData)
val labelsAndData = rawData.map(parseFunction)
val normalizeFunction = normalize(labelsAndData.values)
val normalizedLabelsAndData = labelsAndData.mapValues(normalizeFunction).cache()
(120 to 160 by 10).map(k =>
  (k, clusteringScoreByEntropy(normalizedLabelsAndData, k))).toList.foreach(println)

// -------- Show the Distribution of Labels for the Best Value of k = 150 -----

val kmeans = new KMeans()
kmeans.setK(150)
val model = kmeans.run(normalizedLabelsAndData.values)

val clusterLabelCount = labelsAndData.map {
  case (label, vector) =>
    val cluster = model.predict(normalizeFunction(vector))
    (cluster, label)
}.countByValue()

clusterLabelCount.toSeq.sorted.foreach {
  case ((cluster, label), count) =>
    println(f"$cluster%1s$label%18s$count%8s")
}

normalizedLabelsAndData.unpersist()

// ------------ And Run the Final Anomaly Detection Strategy ------------------

def anomaly(
  data: RDD[Vector],
  normalizeFunction: (Vector => Vector)): (Vector => Boolean) = {

  val normalizedData = data.map(normalizeFunction)
  normalizedData.cache()

  val kmeans = new KMeans()
  kmeans.setK(150)
  val model = kmeans.run(normalizedData)

  normalizedData.unpersist()

  val distances = normalizedData.map(vector => distToCentroid(vector, model))
  val threshold = distances.top(100).last

  (vector: Vector) => distToCentroid(normalizeFunction(vector), model) > threshold
}

val parseFunction = onehot(rawData)
val originalAndData = rawData.map(line => (line, parseFunction(line)._2))
val data = originalAndData.values
val normalizeFunction = normalize(data)
val anomalyFunction = anomaly(data, normalizeFunction)
val anomalies = originalAndData.filter {
  case (original, vector) => anomalyFunction(vector)
}.keys
anomalies.take(10).foreach(println)